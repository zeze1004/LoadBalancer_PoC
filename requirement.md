
## 프로젝트 배경
지금까지는 개별 서비스가 클라우드에서 제공하는 GPT 서비스를 직접 호출했지만
서비스 수요가 증가함에 따라 다음과 같은 문제가 발생했습니다.

- 반복적인 속도 제한 및 재시도 백오프 구현의 비효율성
  - 사용량 증가로 인해 하나의 리전의 할당량이 부족하게 되어 여러 리전의 노드가 필요함
  - 예: Azure OpenAI 서비스의 여러 리전 활용
- 각 환경 및 서비스에 대해 API 키를 관리하는 번거로움
- LLM 사용 분석의 어려움

이러한 문제를 해결하기 위해 **`AI 프록시`**를 개발하려고 합니다.

## 프로젝트 목표
Go 언어로 AI 프록시 기능을 PoC 하는 로드밸런서 프로젝트를 구현합니다.

다수의 노드에 대해 로드 밸런서를 구현하며 다음의 속도 제한을 목표로 합니다.

1. 각 노드는 서로 다른 속도 제한을 가질 수 있습니다.
2. 속도 제한은 두 가지 방식으로 측정됩니다. 
    - `BPM` (http 본문 분당 바이트 수), `RPM` (분당 요청 수)
3. 두 가지 방식 중 하나라도 기준이 넘기면 노드에 요청이 가지 않습니다. 
